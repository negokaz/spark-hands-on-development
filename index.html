<!DOCTYPE html>
<html>

<head>
  <title>Apache Spark Hands-On Development</title>
  <meta charset="utf-8">
  <link rel="shortcut icon" href="resources/favicon.ico">
  <link rel="stylesheet" type="text/css" href="resources/light-theme.css" />
  <link rel="stylesheet" type="text/css" href="resources/tech-circle.css" />
  <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
  <script src="resources/jquery.qrcode.min.js"></script>
</head>

<style>

  .zero-margin * {
    margin: 0;
  }
  .indent {
    margin-left: 3em;
  }

  .remote-controller label {
    color: rgba(0, 0, 0, 0.60);
  }

  @media print {
    .remote-controller {
      visibility: hidden;
    }
  }
</style>

<body>
<div class="remote-controller">
  <label for="enable-sync" class="subscribe-control">スライドを同期:</label>
  <input type="checkbox" id="enable-sync" class="subscribe-control" checked />
</div>
<script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script>
<textarea id="source">
# 前準備

* スライド: [http://bit.ly/1NvuNie](http://negokaz.github.io/spark-hands-on-development/) .small[(Chrome 推奨)]
    * PDF: [http://bit.ly/1OrSgCd](apache-spark-hands-on-development.pdf)

.small[
------

* 無線LANを接続してください
* 最新版のソースを取得してください
    ~~~
    # spark-hands-on-development ディレクトリ直下で実行してください
    # 最新の `step1` ブランチをチェックアウト
    git fetch
    git checkout step1
    ~~~
* 質問や気になることなどはチャットへ投稿してください
    * http://chat.tech-circle-10.mydns.jp/
    * ID: guest / PW: guest でログインしてください
]

???

* Back-channeling を起動しておく

---
class: middle, fit-left, background-cover, tc-title
background-image: url("img/NKJ56_kaigisuruahiruchan500.jpg")

# Apache Spark
## Hands-On Development
### @Tech-Circle #10

.float-bottom-10.glass[
Twitterハッシュタグのランキングを実況する Webアプリを開発してみよう
]

---

## 私は何者？

* 根来 和輝 .small[Negoro Kazuki]
* TIS株式会社 生産技術R＆D室
* Typesafe Reactive Platform の有効性検証
    * Scala / Play / Slick / Akka

.twitter-icon[[@negokaz](https://twitter.com/negokaz)] .github-icon[[negokaz](https://github.com/negokaz)]

???

* Reactive Platform の検証
    * Typesafe がメインで作っている Reactive System を実現するためのフレームワークやライブラリ
    * SI でどのようにすれば活用できるか？

---

## Apache Spark とは？

.center[![spark logo](img/spark-logo-hd.png)]

* ビックデータのための並列分散処理基盤
* 大量にあるデータの集計や分析ができる
* オープンソースで開発されている
* 最新のバージョンは 1.5.1 .small[(2015/11 現在)]

.footnote[http://spark.apache.org/]

???

* Apache の トップレベルプロジェクト
* 近年とても注目されているプロジェクト

---

## Apache Spark とは？

データを処理するための様々なライブラリを提供
.left-column[
.size-100[![](img/spark-stack.png)]
]

.right-column[
* Spark Streaming
  * ストリーム処理
* MLlib
  * 機械学習
* GraphX
  * グラフ処理
]

???

* グラフ処理
    * 最短距離を求める
    * ソーシャルグラフ - 友達の友達は誰？

--

.float-bottom-10.glass-deep[
それぞれのライブラリを組み合わせて 複雑な処理を実装できる。
]

---

## .size-20[![spark](img/spark-logo-hd.png)] vs .size-40[![hadoop](img/hadoop.svg)] ![Spark vs Hadoop](img/logistic-regression.png)

* Hadoopは中間データを**ストレージ**に書き込む
    * スループットを高めることを重視
* Sparkは中間データを**メモリ**に書き込む
    * レイテンシを低くすることを重視
* メモリ総量の数十倍のデータ量を扱う場合は Hadoopが安定

.footnote.small[[Apache Sparkがスループットとレイテンシを両立させた仕組みと<br>最新動向をSparkコミッタとなったNTTデータ猿田氏に聞いた](http://www.publickey1.jp/blog/15/apache_sparksparkntt.html)]

???

* ロジスティック回帰

---

## Sparkの導入事例

* .small[[Spark and Shark Bridges the Gap Between BI and Machine Learning](https://spark-summit.org/2014/talk/spark-and-shark-bridges-the-gap-between-bi-and-machine-learning-at-yahoo-taiwan)]
    * Yahoo! 台湾法人
    * Hadoop MapReduce から Spark への移行
    * 日次レポート作成: 83%↑ 機械学習: 7.5倍↑
* .small[[Real-Time Recommendations using Spark](https://spark-summit.org/east-2015/talk/real-time-recommendations-using-spark)]
    * 米 Comcast Labs.
    * 好みに合わせた番組のリアルタイムレコメンド
    * MLlib と Spark Streaming を組み合わせ

???

## Yahoo 台湾法人

* 日次レポート: BI
* 機械学習: ユーザーへのレコメンドのための学習

## Comcast Labs.

* ユーザーの視聴履歴をMLlibでバッチ処理。ユーザーをクラスタリング。
* Spark Streaming を使ってユーザーのクラスタごとの選局状況を取得し、レコメンド。


---

## Sparkのユースケース

* 1台のサーバーではまかないきれないデータを 集計・分析したい
    * クラスタの総メモリサイズに収まるデータ量
* 将来的にデータ量が増える可能性がある
* スループットよりもレイテンシを重視したい

.footnote[より詳しく: [Apache Sparkが描く大規模インメモリ処理の世界](http://www.slideshare.net/hadoopxnttdata/apache-spark-1000-nodes-ntt-data)]

???

* スケーラリビティ
    * プログラムを書き換えずに分散させる台数を増やせる

---

class: middle, center

# Hands-On

???

#【0:05:00】

---

## これから開発するアプリ

.size-80[![](img/screenshot.png)]

--

.float-top-0.glass-deep[
ハッシュタグのランキングを実況するWebアプリ
* たくさんTweetされた順にハッシュタグを表示
    * 順位、Tweet数 が確認できる
* ハッシュタグをクリックするとTweetが見れる
.right.small[[Twitter - そもそも＃ハッシュタグって何？](https://support.twitter.com/articles/20170159-#whatis)]
]
---

## 構成

.center[.size-80[![](img/app-architecture.svg)]]

---
class: middle, center

# Sparkでプログラミング ①
## 静的データを処理する

???

Sparkの基本的なところを知るためにまずは、静的なデータの解析をしてみましょう。
事前にテキストファイルに落としておいたツイートのデータを解析してみます。

---

## 準備

1. IntelliJ IDEA を起動
1. .accent[SparkLogic.scala] を開く

.small[
* modules > backend > src > main > scala > com.example.tagrank.backend > spark
* Shift × 2 で SparkLogic.scala を入力すると簡単に検索できます
]

???

色んなファイルを見に行くと手間なので、今回触るファイルはこれだけです。

---

## 現在の実装

~~~scala
/**
 * ① テキストファイルからツイートを読み込んで解析
 */
def analyzeRanking(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  // tweets.txt の中にある全ツイートの RDD
* val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // Ranking に変換する RDD
  val rankingsRDD: RDD[Ranking] =
    tweetsRDD map { tweet: String =>
      // String を Ranking ケースクラスに変換
      Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
    }

  // collect() を呼び出すことによって実際の処理が始まる
  val rankings: Array[Ranking] = rankingsRDD.collect()

  // フロントエンドに結果を渡す
  receiver ! rankings
}
~~~

???

* analyzeRanking 何行目？
    * 37行目

--

.float-bottom-10.glass-deep[
* テキストファイルのデータからRDDを作成
* .small[modules/backend/src/main/resources/tweets.txt]
]

---

## RDD ? - .small[Resilient Distributed Datasets]

* 耐障害性のある分散データセット
* イミュータブル
* データはパーティションに分割され 各ノードに分散して配置される
* .accent[入力元のRDD]と.accent[処理内容]の情報を持っているため 壊れても復旧可能

.footnote[
[Apache Spark - Resilient Distributed Datasets (RDDs)](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds)
([Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD))
]

???

* イミュータブル
    * 一旦作ったら、変更できない
    * 別のデータを持つRDDが欲しい場合は、変更したいRDDを元に新しいRDDを作る
* 一部のノードに障害が起きてデータが一部欠損しても復旧することができる

---

## 現在の実装

~~~scala
/**
 * ① テキストファイルからツイートを読み込んで解析
 */
def analyzeRanking(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  // tweets.txt の中にある全ツイートの RDD
  val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // Ranking に変換する RDD
* val rankingsRDD: RDD[Ranking] =
*   tweetsRDD map { tweet: String =>
*     // String を Ranking ケースクラスに変換
*     Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
*   }

  // collect() を呼び出すことによって実際の処理が始まる
  val rankings: Array[Ranking] = rankingsRDD.collect()

  // フロントエンドに結果を渡す
  receiver ! rankings
}
~~~

--

.float-bottom-50.glass-deep[
* `map()` で `String` ⇒ `Ranking` に変換する
* *Ranking* : hashTag, rank, sampleTweets, sampleCount を属性にもつ
]

???

* Ranking
    * 1つのハッシュタグの順位を表現するケースクラス
    * フロントエンドに渡すデータ
    * hashTag
        * 1つのハッシュタグ
    * rank
        * そのハッシュタグの順位
    * sampleCount
        * そのハッシュタグを含むツイートの数
    * sampleTweets
        * ハッシュタグが含まれていたツイートの数

---

## 現在の実装

~~~scala
/**
 * ① テキストファイルからツイートを読み込んで解析
 */
def analyzeRanking(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  // tweets.txt の中にある全ツイートの RDD
  val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // Ranking に変換する RDD
  val rankingsRDD: RDD[Ranking] =
    tweetsRDD map { tweet: String =>
      // String を Ranking ケースクラスに変換
      Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
    }

  // collect() を呼び出すことによって実際の処理が始まる
* val rankings: Array[Ranking] = rankingsRDD.collect()

  // フロントエンドに結果を渡す
  receiver ! rankings
}
~~~

--

.float-bottom-30.glass-deep[
* RDD の `collect()` を呼び出すことで RDD に定義した処理が初めて実行される
  * テキストファイルの読み込み
  * `map()` による `Ranking` への変換
]

???
* RDDの宣言だけでは実行されないということに注意
* 実際に処理が実行されるメソッド ⇒ **アクション**
    * RDDの要素数を数える`count()`などがある

---

## 現在の実装

~~~scala
/**
 * ① テキストファイルからツイートを読み込んで解析
 */
def analyzeRanking(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  // tweets.txt の中にある全ツイートの RDD
  val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // Ranking に変換する RDD
  val rankingsRDD: RDD[Ranking] =
    tweetsRDD map { tweet: String =>
      // String を Ranking ケースクラスに変換
      Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
    }

  // collect() を呼び出すことによって実際の処理が始まる
  val rankings: Array[Ranking] = rankingsRDD.collect()

  // フロントエンドに結果を渡す
* receiver ! rankings
}
~~~

--

.float-bottom-30.glass-deep[
* フロントエンドへ結果を返す
    * ここでは[Akka](http://akka.io/)を使って結果を送っている
    * ただの配列になっているので他の方法でもOK
        * REST APIにする・DBに保存 etc...
]

???

`Ranking`の配列」を解析結果としてフロントエンドに返す

---

## アプリケーションの起動

```bash
### Mac OS X###
cd spark-hands-on-development
./activator backend/run
./activator run
```
```bash
### Windows ###
cd spark-hands-on-development
activator backend/run
activator run
```

▶ ブラウザで http://localhost:9000/ を開く

???

* 全部 ×1
* 同じハッシュタグがサマリされておらずランキングになっていない
* 日本語以外の読めない言語が表示されている

---

## コンソール上で動きを確認

* 毎回ブラウザで確認するのは手間なので コマンドを用意しておきました

~~~bash
### Mac OS X ###
./activator backend/print
~~~

~~~bash
### Windows ###
activator backend/print
~~~

▶ `Ctrl + C` で終了

???

これからこのプログラムを修正していきます。
手間なので、コマンドを用意しておきました。
IntelliJ上で確認してみてください。

---

## 日本語のツイートを抽出 - .small[使うもの]

* `RDD#filter()` を使う
* `filter()` には 「`String` を引数にとって `Boolean` を返す関数」を渡す
  * .accent[true] になる要素だけの RDD が生成される

.footnote[[Scala API - RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)]

* `def containsJapaneseChar(s: String): Boolean`<br> を用意しておきました

▶ `tweetsRDD` に `filter()` を適用してみる

---

## 日本語のツイートを抽出 - .small[実装]

~~~scala
  // tweets.txt の中にある全ツイートの RDD
  val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // 日本語を含むツイートを抽出する RDD
* val japaneseTweetsRDD: RDD[String] =
*   tweetsRDD.filter(containsJapaneseChar)

  // Ranking に変換する RDD
  val rankingsRDD: RDD[Ranking] =
    tweetsRDD map { tweet: String =>
      // String を Ranking ケースクラスに変換
      Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
    }
~~~
* `tweetsRDD.filter()` に `containsJapaneseChar`<br>を渡す

---

## 日本語のツイートを抽出 - .small[実装]

~~~scala
  // tweets.txt の中にある全ツイートの RDD
  val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // 日本語を含むツイートを抽出する RDD
  val japaneseTweetsRDD: RDD[String] =
    tweetsRDD.filter(containsJapaneseChar)

  // Ranking に変換する RDD
  val rankingsRDD: RDD[Ranking] =
*   japaneseTweetsRDD map { tweet: String =>
      // String を Ranking ケースクラスに変換
      Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
    }
~~~
* `Ranking` へは `japaneseTweetsRDD` から変換するように書き換えておく

---

## ハッシュタグを抽出 - .small[設計]

~~~scala
Ranking("#hashTag", rank = ???, Array(tweet), sampleCount = ???)
~~~

.accent[ハッシュタグ]とそのハッシュタグを含んでいる .accent[ツイートの配列]のペアを作らないといけない

???

Ranking: フロントエンドに結果として返すオブジェクト

---

## ハッシュタグを抽出 - .small[設計]

.zero-margin[
~~~scala
// japaneseTweetsRDD: RDD[String]
"ツイートA #hashTag1"
"ツイートB #hashTag1 #hashTag2"
~~~
.indent[▼ .small[ハッシュタグとツイートのペアを作成]]
~~~scala
("#hashTag1", "ツイートA #hashTag1")
("#hashTag1", "ツイートB #hashTag1 #hashTag2")
("#hashTag2", "ツイートB #hashTag1 #hashTag2")
~~~
.indent[▼ .small[同じハッシュタグでグループ分け]]
~~~scala
("#hashTag1", ["ツイートA #hashTag1", "ツイートB #hashTag1 #hashTag2"])
("#hashTag2", ["ツイートB #hashTag1 #hashTag2"])
~~~
.indent[▼ .small[`Ranking`に変換]]
~~~scala
Ranking("#hashTag1", rank = ???, ["ツイートA #hashTag1", "ツイートB #hashTag1 #hashTag2"], sampleCount = ???)
Ranking("#hashTag2", rank = ???, ["ツイートB #hashTag1 #hashTag2"], sampleCount = ???)
~~~
]

---

## ハッシュタグを抽出 - .small[使うもの]
.zero-margin[
~~~scala
// japaneseTweetsRDD: RDD[String]
"ツイートA #hashTag1"
"ツイートB #hashTag1 #hashTag2"
~~~
.indent[▼ .small[ハッシュタグとツイートのペアを作成]]
~~~scala
("#hashTag1", "ツイートA #hashTag1")
("#hashTag1", "ツイートB #hashTag1 #hashTag2")
("#hashTag2", "ツイートB #hashTag1 #hashTag2")
~~~
]
--

* `RDD#flatMap()`, `RDD#map()` を使う
    * for式でシンプルに記述できる
* `def pickHashTags(s: String): Set[String]`<br>を用意しておきました

.footnote[[Scala API - RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)]

---

## ハッシュタグを抽出 - .small[実装]

~~~scala
// flatMap, map を使った場合
val hashTagTweetPairRDD: RDD[(String, String)] =
  japaneseTweetsRDD flatMap { tweet =>
    pickHashTags(tweet) map { hashTag =>
      (hashTag, tweet)
    }
  }
// for式を使った場合
val hashTagTweetPairRDD: RDD[(String, String)] =
  for {
    tweet   <- japaneseTweetsRDD
    hashTag <- pickHashTags(tweet)
  } yield (hashTag, tweet)
~~~
~~~scala
// Ranking に変換する RDD
val rankingsRDD: RDD[Ranking] =
* hashTagTweetPairRDD map { case (hashTag, tweet) =>
*   Ranking(hashTag, rank = 1, Array(tweet), sampleCount = 1)
  }
~~~

---

## ハッシュタグでグループ分け - .small[使うもの]

.zero-margin[
~~~scala
("#hashTag1", "ツイートA #hashTag1")
("#hashTag1", "ツイートB #hashTag1 #hashTag2")
("#hashTag2", "ツイートB #hashTag1 #hashTag2")
~~~
.indent[▼ .small[同じハッシュタグでグループ分け]]
~~~scala
("#hashTag1", ["ツイートA #hashTag1", "ツイートB #hashTag1 #hashTag2"])
("#hashTag2", ["ツイートB #hashTag1 #hashTag2"])
~~~
]
* `RDD#groupByKey()`
    * タプルの1番目をキーとしてグループ分けされる

.footnote[[Scala API - RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)]

---

## ハッシュタグでグループ分け - .small[実装]

~~~scala
val hashTagGroupsRDD: RDD[(String, Iterable[String])] =
  hashTagTweetPairRDD.groupByKey()
~~~
~~~scala
// Ranking に変換する RDD
val rankingsRDD: RDD[Ranking] =
* hashTagGroupsRDD map { case (hashTag, tweets) =>
*   Ranking(hashTag, rank = 1, tweets.toArray, sampleCount = tweets.size)
  }
~~~
* `sampleCount`には`tweets.size`を設定しておく

---

## ツイートの多い順にソート

* `RDD#sortBy()` を使う
* `tweets.size` の降順にしておく

~~~scala
// ツイートの数でソートする RDD
val sortedHashTagGroupsRDD: RDD[(String, Iterable[String])] =
  hashTagGroupsRDD.sortBy({ case (_, tweets) =>
    tweets.size
  }, ascending = false)
~~~
~~~scala
// Ranking に変換する RDD
val rankingsRDD: RDD[Ranking] =
* sortedHashTagGroupsRDD map { case (hashTag, tweets) =>
    Ranking(hashTag, rank = 1, tweets.toArray, sampleCount = tweets.size)
  }
~~~

.footnote[[Scala API - RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)]

---

## ランクを設定

* `RDD#zipWithIndex()`
* index は 0 始まりなので + 1 しておく

~~~scala
// ソートされた各要素にインデックスを付ける RDD
val rankedHashTagGroupsRDD: RDD[((String, Iterable[String]), Long)] =
  sortedHashTagGroupsRDD.zipWithIndex()
~~~
~~~scala
// Ranking に変換する RDD
val rankingsRDD: RDD[Ranking] =
* rankedHashTagGroupsRDD map { case ((hashTag, tweets), index) =>
*   Ranking(hashTag, rank = index + 1, tweets.toArray, sampleCount = tweets.size)
}
~~~

.footnote[[Scala API - RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)]

???

終了: 【0:23:00】

---

## 解答

実装が間に合わなかった場合は下記コマンドで 解答をチェックアウトしてください

~~~bash
# spark-hands-on-development ディレクトリの直下で実行してください
git commit -am "「Sparkでプログラミング ①」の途中まで実装"
git checkout step2
~~~
.small[※ 途中の作業は `step1` ブランチに保存されます]

---
class: middle, center

# Sparkでプログラミング ②
## ストリームデータを処理する

???

#【0:25:00】

---

## 現在の実装

~~~scala
/**
 * ② Spark Streams を使ってリアルタイムにツイートを解析
 */
def analyzeRankingWithStream(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  // Twitter の DStream
  val twitterStream: ReceiverInputDStream[Status] =
    TwitterUtils.createStream(ssc, None)

  // ツイート の DStream に変換
  val tweetStream: DStream[String] =
    twitterStream map { status =>
      status.getText
    }

  // ストリームの塊を処理する
  tweetStream.foreachRDD { rdd: RDD[String] =>
  /* …省略… */
}
~~~

???

`analyzeRankingWithStream`: `analyzeRanking`の下

--

.float-bottom-60.glass-deep[
* DStream の API を通してストリームを操作する
* `foreachRDD`でストリームから`RDD`を取り出せる
]

---

## DStream ? - Discretized Stream

* 離散化ストリーム
* 連続するRDDの集合
    * RDDはバッチインターバルごとの入力データを持つ
    * バッチインターバルは`StreamingContext`に設定
* RDDと似たAPIを持つ

.footnote[[Apache Spark - Discretized Streams (DStreams)](http://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams)([Scala API](https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/streaming/dstream/DStream.html))]

???

* バッチインターバル: 500ミリ秒

---

## 現在の実装

~~~scala
/**
 * ② Spark Streams を使ってリアルタイムにツイートを解析
 */
def analyzeRankingWithStream(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  /* …省略… */

  // ストリームの塊を処理する
  tweetStream.foreachRDD { rdd: RDD[String] =>
      // Ranking に変換する RDD
      val rankingsRDD = rdd map { tweet: String =>
          // String を Ranking ケースクラスに変換
          Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
      }
      // collect() を呼び出すことによって実際の RDD の処理が始まる
      val rankings = rankingsRDD.collect()

      receiver ! rankings
    }

  // start() を呼び出すことによって上記で定義した Stream の処理が始まる
  ssc.start()
}

~~~

--

.float-bottom-50.glass-deep[
* `foreachRDD`で取り出したRDDには バッチインターバルのデータが含まれている
* バッチインターバル以上の時間幅の集計も可能
]

---

## バッチインターバル以上の集計

.zero-margin[
~~~scala
// ペアの1番目をキーとしてツイートのグループを作る DStream
val hashTagGroupsStream: DStream[(String, Iterable[String])] =
  hashTagTweetPairStream.groupByKey()
~~~
* ⇒ バッチインターバル(500ミリ秒) ごとのツイートが集計される
* 過去何分かの集計を表示したい場合は `DStream#groupByKeyAndWindow`を使って ウィンドウ集計する。↓
~~~scala
// ペアの1番目をキーとしてツイートのグループを作る DStream
val hashTagGroupsStream: DStream[(String, Iterable[String])] =
  hashTagTweetPairStream
      .groupByKeyAndWindow(windowDuration = Minutes(1), slideDuration = Seconds(1))
~~~
]

???

静的なファイルの解析のときを思い出して、集計処理は groupByKey 使いましたよね？

---

## ウィンドウ集計

.size-80[![](img/streaming-window.svg)]

???

* 時間に任意の幅を持たせて集計できるようになる
* 例えば「過去1分間のハッシュタグのランキングを1秒ごとに求めたい」
    * ⇒ window幅 に Minutes(1), slide幅 に Seconds(1) を指定する
    * window幅とslide幅はバッチインターバルの倍数である必要がある
* ブルーの箱の幅がバッチインターバル
* 薄いオレンジの破線の枠が最初の集計グループ
* 濃いオレンジの枠が次の集計グループ


---
## 実装してみよう

* `DStream#filter`で日本語のツイートを抽出
* `for` or `DStream#map`, `DStream#flatMap` でハッシュタグ・ツイートのペアを作成
* `DStream#groupByKeyAndWindow`で 過去1分間の集計を1秒ごとに行う
* 集計したものを`DStream#foreachRDD`の中で
    * ソートしインデックスを付与
    * `Ranking`に変換

???

終了: 0:42:00

---

## 実装してみよう

`analyzeLogic`を書き換えて確認してください

.zero-margin[
~~~scala
val analyzeLogic: analyzeLogicType = analyzeRanking
~~~
.center[▼]
~~~scala
val analyzeLogic: analyzeLogicType = analyzeRankingWithStream
~~~
]

???

* `analyzeLogic`何行目?
    * 19行目

---


## 解答

実装が間に合わなかった場合は下記コマンドで 解答をチェックアウトしてください

~~~bash
# spark-hands-on-development ディレクトリの直下で実行してください
git commit -am "「Sparkでプログラミング ②」の途中まで実装"
git checkout step3
~~~
.small[※ 途中の作業は `step2` ブランチに保存されます]

---
class: middle, center

# Sparkのクラスタを作る

???

#【0:45:00】

---

## Sparkのクラスタアーキテクチャ

.left-column[
.size-100[![](img/cluster-overview.png)]

.footnote[http://spark.apache.org/docs/latest/cluster-overview.html]
]
.right-column.small[
* Driver Program
    * アプリケーションのメインプロセス。SparkContextを起動する。
* SparkContext
    * クラスタへアクセスするための入り口となるオブジェクト。これからRDDを作る。
* Cluster Manager
    * クラスタのリソースを管理する。
* Worker Node
    * アプリケーションコードが実行されるノード。
* Executor
    * ワーカーノード上で起動するプロセス。Taskを実行したり中間データを保持する。
* Task
    * Executor上で実行される処理。
]

???

http://spark.apache.org/docs/latest/spark-standalone.html#launching-spark-applications

1. Driver Program から filter, map などのタスクが Worker Node の Executor に送られます
1. Worker上の Executor プロセスで Task が実行されて
1. 結果が Spark Context に返されます
1. Cluster Manager が処理のジョブスケジューリングを行います

* Driver は ワーカーノードのどれかで起動する

---

## 手順

1. バックエンドの JAR(Java Archive) を作成
1. Sparkのクラスタを起動
    * Master
    * Worker
1. JAR を Spark のクラスタへ submit

---

## バックエンドのJARを作成

```bash
### Mac OS X ###
cd spark-hands-on-development
# JAR を作成
./activator backend/assembly
# JAR が作成できていることを確認
ls modules/backend/target/scala-2.10/
```

```bash
### Windows ###
cd spark-hands-on-development
# JAR を作成
activator backend/assembly
# JAR が作成できていることを確認
dir modules\backend\target\scala-2.10\
```
.small[▶ twitter-hashtag-ranking-backend-assembly-1.0.0.jar]

---

## クラスタの起動 - Master

```bash
spark-class org.apache.spark.deploy.master.Master
```

```bash
...
* 15/10/13 12:44:34 INFO Master: Starting Spark master at spark://192.168.179.3:7077
...
```
* 起動時のログに Master のアドレスが出力される

???

Masterを複数立ち上げる場合は Zookeeper を使う
https://spark.apache.org/docs/latest/spark-standalone.html#standby-masters-with-zookeeper

---

## クラスタの起動 - Workers

```bash
spark-class org.apache.spark.deploy.worker.Worker spark://192.168.179.3:7077
```

* 引数にMasterのアドレスを指定する
* Workerはいくつでも立ち上げることができる

---

## クラスタへの submit

```bash
# JAR があるディレクトリに移動
cd spark-hands-on-development/modules/backend/target/scala-2.10/
# Sparkのクラスタへ submit
spark-submit --deploy-mode cluster --master spark://192.168.179.3:7077 twitter-hashtag-ranking-backend-assembly-1.0.0.jar
```

---

## 画面を確認

フロントエンドの起動
~~~bash
### Mac OS X ###
cd spark-hands-on-development
./activator run
~~~
~~~bash
### Windows ###
cd spark-hands-on-development
activator run
~~~

▶ブラウザで http://localhost:9000/ を開く

---

## クラスタマネージャーの種類

* [Spark Standalone](http://spark.apache.org/docs/latest/spark-standalone.html)
    * Sparkのパッケージに含まれるクラスタマネージャ。 簡単にクラスタを構築できる。
* [Apache Mesos](http://spark.apache.org/docs/latest/running-on-mesos.html)
    * 汎用クラスタマネージャ。
* [Hadoop YARN](http://spark.apache.org/docs/latest/running-on-yarn.html)
    * Hadoopのリソースマネージャ。

.footnote[[Cluster Manager Types](http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types)]
---

## まとめ

* 分散処理が簡単にプログラミングできる
* ライブラリを組み合わせることで実現できることの 幅が広がる
* プログラムを書き換えることなくスケールアウト できる

---
class: middle, center
# 参考情報

???

#【0:55:00】

* 日本語の書籍紹介

---

## 書籍: 初めてのSpark

.center[.size-30[![](img/learning-spark.jpg)]]
.footnote[https://www.oreilly.co.jp/books/9784873117348/]

???

---

## 書籍: Apache Spark入門

.center.size-30[![](img/spark-japanese-book.jpg)]
.footnote[https://www.shoeisha.co.jp/book/detail/9784798142661]

---

## TIS㈱ リアクティブ・システム<br>コンサルティングサービス

* Typesafe のコンサルティングパートナー
* Reactive Platform .small[(Play / Akka / Slick / Scala)]
  * 技術検証
  * 設計レビュー
  * コードレビュー
  * システム構築

.footnote[
.small[※ Spark は近い将来ラインナップします！！]

https://www.tis.jp/service_solution/goreactive/
]

---
class: middle, center

# Thank you!

---

## お願い

.center[
### アンケートにご協力ください
.qr-code.size-20[http://goo.gl/forms/wF3zZzRw6O]<br>
[回答する](http://goo.gl/forms/wF3zZzRw6O)
]

</textarea>

<script>
  var slideshow = remark.create({
          highlightStyle: 'monokai',
          highlightLanguage: 'remark',
          highlightLines: true
        }) ;

  $(function() {
      // 全てのリンクが別のタブで開くようにする
      $('a').attr({target: "_blank"}); //_

      // QR-Code support
      $(".qr-code").each(function() {
        var $this = $(this);
        var text = $this.text();
        $this.text("");
        $this.qrcode(text);
      });


      // var url = "ws://localhost:9000";
      var url = "wss://stark-dusk-4378.herokuapp.com";

      var connectAsController = function() {

        $(".subscribe-control").hide();

        var passcode = window.prompt("passcode?", "");

        if (passcode === null) {
          return;
        }

        var innerConnect = function() {

          var connection = new WebSocket(url + "/control?passcode=" + passcode);

          connection.onopen = function() {
            slideshow.on('showSlide', function(slide) {
              var index = slide.getSlideIndex();
              connection.send(JSON.stringify({index: index}));
            });
          };
          connection.onmessage = function(e) {
          };
          connection.onerror = function(error) {
            console.log(error);
            alert("通信でエラーが発生しました。同期を有効にするにはリロードする必要があります。");
            // disable retry
            connection.onclose = function() {
              console.log("closed");
            };
          };
          connection.onclose = function() {
            console.log("closed");
            // retry
            setTimeout(function() {
              innerConnect();
            }, 1000);
          };
        };
        innerConnect();
      };

      var connectAsSubscriber = function() {

          var connection = new WebSocket(url + "/subscribe");

          connection.onopen = function() {
            console.log("connected remote controller");
            $(".subscribe-control").show();
            connection.send(JSON.stringify({request: "latestCommand"}));
          };
          connection.onmessage = function(e) {
            var data = JSON.parse(e.data);

            if ($("#enable-sync").prop("checked")) {
              if (data.index) {
                slideshow.gotoSlide(data.index + 1);
              }
            }
          };
          connection.onerror = function(error) {
            console.log(error);
            alert("通信でエラーが発生しました。同期を有効にするにはリロードする必要があります。");
            // disable retry
            connection.onclose = function() {
              console.log("closed");
            };
          };
          connection.onclose = function() {
            console.log("closed");
            // retry
            setTimeout(function() {
              connectAsSubscriber();
            }, 1000);
          };
      };

      if (window.location.search && window.location.search == "?control=true") {
        connectAsController();
      } else {
        connectAsSubscriber();
      }
  });

</script>
</body>
</html>
