<!DOCTYPE html>
<html>

<head>
  <title>Apache Spark Hands-On Development</title>
  <meta charset="utf-8">
  <link rel="shortcut icon" href="resources/favicon.ico">
  <link rel="stylesheet" type="text/css" href="resources/light-theme.css" />
  <link rel="stylesheet" type="text/css" href="resources/tech-circle.css" />
  <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
  <script src="resources/jquery.qrcode.min.js"></script>
</head>

<style>

  .zero-margin * {
    margin: 0;
  }
  .indent {
    margin-left: 3em;
  }

  .remote-controller label {
    color: rgba(0, 0, 0, 0.60);
  }

  .todo {
    position: absolute;
    right: 1em;
    font-size: 0.4em;
    color: rgba(0, 0, 0, 0.60);
    padding: 0 0.5em;
    border: 1px solid rgba(0,0,0,0.2);
  }

  .todo ul::before, .todo ol::before {
    content: "TODO";
    color: rgba(0, 0, 0, 0.40);
    font-weight: bold;
    margin-bottom: 0.5em;
  }

  .todo ul, ol {
    margin: 0.8em 0;
  }

  .todo ul {
    list-style: none;
    padding: 0 0.1em;
  }

  .todo input {
    margin: 0 0.5em;
  }

  @media print {
    .remote-controller {
      visibility: hidden;
    }
  }
</style>

<body>
<div class="remote-controller">
  <label for="enable-sync" class="subscribe-control">スライドを同期:</label>
  <input type="checkbox" id="enable-sync" class="subscribe-control" checked />
</div>
<script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script>
<script src="resources/reconnecting-websocket.min.js"></script>
<script src="resources/remark-remote.js"></script>
<textarea id="source">
.todo[
* 前準備を全て実施
]

# 前準備

* 無線LANに接続
* スライドを開く: [http://bit.ly/1NvuNie](http://negokaz.github.io/spark-hands-on-development/) .small[(Chrome 推奨)]
    * IEしか無い場合 → PDF: [http://bit.ly/1OrSgCd](apache-spark-hands-on-development.pdf)

.small[
------

* 最新版のソースを取得してください
    ~~~
    # spark-hands-on-development ディレクトリ直下で実行してください
    # 最新の `step1` ブランチをチェックアウト
    git fetch
    git checkout step1
    ~~~
* 質問や気になることなどはチャットへ投稿してください
    * http://chat.tech-circle-10.mydns.jp/
    * ID: guest / PW: guest でログインしてください
]

???

* Back-channeling を起動しておく

---
class: middle, fit-left, background-cover, tc-title
background-image: url("img/NKJ56_kaigisuruahiruchan500.jpg")

# Apache Spark
## Hands-On Development
### @Tech-Circle #10

.float-bottom-10.glass[
Twitterハッシュタグのランキングを実況する Webアプリを開発してみよう
]

---

## 私は何者？

* 根来 和輝 .small[Negoro Kazuki]
* TIS株式会社 生産技術R＆D室
* Typesafe Reactive Platform の有効性検証
    * Scala / Play / Slick / Akka

.twitter-icon[[@negokaz](https://twitter.com/negokaz)] .github-icon[[negokaz](https://github.com/negokaz)]

???

* Reactive Platform の検証
    * Typesafe がメインで作っている Reactive System を実現するためのフレームワークやライブラリ
    * SI でどのようにすれば活用できるか？

---

## Apache Spark とは？

.center[![spark logo](img/spark-logo-hd.png)]

* ビックデータのための並列分散処理基盤
* 大量にあるデータの集計や分析ができる
* オープンソースで開発されている
* 最新のバージョンは 1.5.2 .small[(2015/11 現在)]

.footnote[http://spark.apache.org/]

???

* Apache の トップレベルプロジェクト
* 近年とても注目されているプロジェクト

---

## Apache Spark とは？

データを処理するための様々なライブラリを提供
.left-column[
.size-100[![](img/spark-stack.png)]
]

.right-column[
* Spark Streaming
  * ストリーム処理
* MLlib
  * 機械学習
* GraphX
  * グラフ処理
]

???

* グラフ処理
    * 最短距離を求める
    * ソーシャルグラフ - 友達の友達は誰？

--

.float-bottom-10.glass-deep[
それぞれのライブラリを組み合わせて 複雑な処理を実装できる。
]

---

## .size-20[![spark](img/spark-logo-hd.png)] vs .size-40[![hadoop](img/hadoop.svg)] ![Spark vs Hadoop](img/logistic-regression.png)

* Hadoopは中間データを**ストレージ**に書き込む
    * スループットを高めることを重視
* Sparkは中間データを**メモリ**に書き込む
    * レイテンシを低くすることを重視
* メモリ総量の数十倍のデータ量を扱う場合は Hadoopが安定

.footnote.small[[Apache Sparkがスループットとレイテンシを両立させた仕組みと<br>最新動向をSparkコミッタとなったNTTデータ猿田氏に聞いた](http://www.publickey1.jp/blog/15/apache_sparksparkntt.html)]

???

* ロジスティック回帰

---

## Sparkの導入事例

* .small[[Spark and Shark Bridges the Gap Between BI and Machine Learning](https://spark-summit.org/2014/talk/spark-and-shark-bridges-the-gap-between-bi-and-machine-learning-at-yahoo-taiwan)]
    * Yahoo! 台湾法人
    * Hadoop MapReduce から Spark への移行
    * 日次レポート作成: 83%↑ 機械学習: 7.5倍↑
* .small[[Real-Time Recommendations using Spark](https://spark-summit.org/east-2015/talk/real-time-recommendations-using-spark)]
    * 米 Comcast Labs.
    * 好みに合わせた番組のリアルタイムレコメンド
    * MLlib と Spark Streaming を組み合わせ

???

## Yahoo 台湾法人

* 日次レポート: BI
* 機械学習: ユーザーへのレコメンドのための学習

## Comcast Labs.

* ユーザーの視聴履歴をMLlibでバッチ処理。ユーザーをクラスタリング。
* Spark Streaming を使ってユーザーのクラスタごとの選局状況を取得し、レコメンド。


---

## Sparkのユースケース

* 1台のサーバーではまかないきれないデータを 集計・分析したい
    * クラスタの総メモリサイズに収まるデータ量
* 将来的にデータ量が増える可能性がある
* スループットよりもレイテンシを重視したい

.footnote[より詳しく: [Apache Sparkが描く大規模インメモリ処理の世界](http://www.slideshare.net/hadoopxnttdata/apache-spark-1000-nodes-ntt-data)]

???

* スケーラリビティ
    * プログラムを書き換えずに分散させる台数を増やせる

---

class: middle, center

# Hands-On

???

#【0:05:00】

---

## これから開発するアプリ

.size-80[![](img/screenshot.png)]

---
* ハッシュタグのランキングを実況するWebアプリ
* たくさんTweetされた順にハッシュタグを表示
    * 順位、Tweet数 が確認できる
* ハッシュタグをクリックするとTweetが見れる
.size-90[![](img/screenshot-zoom.png)]

.footnote[[Twitter - そもそも＃ハッシュタグって何？](https://support.twitter.com/articles/20170159-#whatis)]

---

## 構成

.center[.size-80[![](img/app-architecture.svg)]]

---
class: middle, center

# Sparkでプログラミング ①
## 静的データを処理する

???

Sparkの基本的なところを知るためにまずは、静的なデータの解析をしてみましょう。
事前にテキストファイルに落としておいたツイートのデータを解析してみます。

---

.todo[
* SparkLogic.scalaを開く
]

## 準備

1. IntelliJ IDEA を起動
1. .accent[SparkLogic.scala] を開く

.small[
* Shift × 2 で SparkLogic.scala を入力すると簡単に検索できます
* modules > backend > src > main > scala > com.example.tagrank.backend > spark
]

???

色んなファイルを見に行くと手間なので、今回触るファイルはこれだけです。

---

## 現在の実装

~~~scala
/**
 * ① テキストファイルからツイートを読み込んで解析
 */
def analyzeRanking(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  // tweets.txt の中にある全ツイートの RDD
* val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // Ranking に変換する RDD
  val rankingsRDD: RDD[Ranking] =
    tweetsRDD map { tweet: String =>
      // String を Ranking ケースクラスに変換
      Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
    }

  // collect() を呼び出すことによって実際の処理が始まる
  val rankings: Array[Ranking] = rankingsRDD.collect()

  // フロントエンドに結果を渡す
  receiver ! rankings
}
~~~

???

* analyzeRanking 何行目？
    * 37行目

--

.float-bottom-10.glass-deep[
* テキストファイルのデータからRDDを作成
* .small[modules/backend/src/main/resources/tweets.txt]
]

---

## RDD ? - .small[Resilient Distributed Datasets]

* 耐障害性のある分散データセット
* イミュータブル
* データはパーティションに分割され 各ノードに分散して配置される
* .accent[入力元のRDD]と.accent[処理内容]の情報を持っているため 壊れても復旧可能

.footnote[
[Apache Spark - Resilient Distributed Datasets (RDDs)](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds)
([Scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD))
]

???

* イミュータブル
    * 一旦作ったら、変更できない
    * 別のデータを持つRDDが欲しい場合は、変更したいRDDを元に新しいRDDを作る
* 一部のノードに障害が起きてデータが一部欠損しても復旧することができる

---

## 現在の実装

~~~scala
/**
 * ① テキストファイルからツイートを読み込んで解析
 */
def analyzeRanking(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  // tweets.txt の中にある全ツイートの RDD
  val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // Ranking に変換する RDD
* val rankingsRDD: RDD[Ranking] =
*   tweetsRDD map { tweet: String =>
*     // String を Ranking ケースクラスに変換
*     Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
*   }

  // collect() を呼び出すことによって実際の処理が始まる
  val rankings: Array[Ranking] = rankingsRDD.collect()

  // フロントエンドに結果を渡す
  receiver ! rankings
}
~~~

--

.float-bottom-50.glass-deep[
* `map()` で `String` ⇒ `Ranking` に変換する
* *Ranking* : hashTag, rank, sampleTweets, sampleCount を属性にもつ
]

???

* Ranking
    * 1つのハッシュタグの順位を表現するケースクラス
    * フロントエンドに渡すデータ
    * hashTag
        * 1つのハッシュタグ
    * rank
        * そのハッシュタグの順位
    * sampleCount
        * そのハッシュタグを含むツイートの数
    * sampleTweets
        * ハッシュタグが含まれていたツイートの数

---

## 現在の実装

~~~scala
/**
 * ① テキストファイルからツイートを読み込んで解析
 */
def analyzeRanking(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  // tweets.txt の中にある全ツイートの RDD
  val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // Ranking に変換する RDD
  val rankingsRDD: RDD[Ranking] =
    tweetsRDD map { tweet: String =>
      // String を Ranking ケースクラスに変換
      Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
    }

  // collect() を呼び出すことによって実際の処理が始まる
* val rankings: Array[Ranking] = rankingsRDD.collect()

  // フロントエンドに結果を渡す
  receiver ! rankings
}
~~~

--

.float-bottom-30.glass-deep[
* RDD の `collect()` を呼び出すことで RDD に定義した処理が初めて実行される
  * テキストファイルの読み込み
  * `map()` による `Ranking` への変換
]

???
* RDDの宣言だけでは実行されないということに注意
* 実際に処理が実行されるメソッド ⇒ **アクション**
    * RDDの要素数を数える`count()`などがある

---

## 現在の実装

~~~scala
/**
 * ① テキストファイルからツイートを読み込んで解析
 */
def analyzeRanking(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  // tweets.txt の中にある全ツイートの RDD
  val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // Ranking に変換する RDD
  val rankingsRDD: RDD[Ranking] =
    tweetsRDD map { tweet: String =>
      // String を Ranking ケースクラスに変換
      Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
    }

  // collect() を呼び出すことによって実際の処理が始まる
  val rankings: Array[Ranking] = rankingsRDD.collect()

  // フロントエンドに結果を渡す
* receiver ! rankings
}
~~~

--

.float-bottom-30.glass-deep[
* フロントエンドへ結果を返す
    * ここでは[Akka](http://akka.io/)を使って結果を送っている
    * ただの配列になっているので他の方法でもOK
        * REST APIにする・DBに保存 etc...
]

???

`Ranking`の配列」を解析結果としてフロントエンドに返す

---

.todo[
* "#hashTag"がたくさん表示されているのを確認
* backendを`Ctrl+C`で終了
]

## アプリケーションの起動

```bash
cd spark-hands-on-development
./activator backend/run
# ↓のログが出力されると起動完了
[INFO] [11/11/2015 00:00:00.000] [backend-akka.actor ... === RankingAnalyzer started ===
```
.zero-margin[
```bash
# もう一つコンソールを開いて↓を実行
cd spark-hands-on-development
./activator run
# ↓のログが出力されると起動完了
--- (Running the application, auto-reloading is enabled) ---

[info] p.c.s.NettyServer - Listening for HTTP on /0:0:0:0:0:0:0:0:9000

(Server started, use Ctrl+D to stop and go back to the console...)
```
]
.small[▶ ブラウザで http://localhost:9000/ を開く]

.footnote[
※ Windowsの場合、`activator`の前の`./`は不要です
]
???

* 全部 ×1
* 同じハッシュタグがサマリされておらずランキングになっていない
* 日本語以外の読めない言語が表示されている

---

.todo[
* backend/printを実行
]

## コンソール上で動きを確認

* 毎回ブラウザで確認するのは手間なので コマンドを用意しておきました

~~~bash
./activator backend/print
~~~
~~~
Ranking(#hashTag, rank: 1, [RT @Mansel~], sampleCount: 1)
Ranking(#hashTag, rank: 1, [RT @mained~], sampleCount: 1)
Ranking(#hashTag, rank: 1, [Can you re~], sampleCount: 1)
Ranking(#hashTag, rank: 1, [初手から首都高とはキ~], sampleCount: 1)
Ranking(#hashTag, rank: 1, [RT @ganppp~], sampleCount: 1)
~~~

.small[▶ `Ctrl + C` で終了]

???

これからこのプログラムを修正していきます。
手間なので、コマンドを用意しておきました。
IntelliJ上で確認してみてください。

---

## 実施する修正

1. 日本語のツイートを抽出
2. ハッシュタグを抽出
3. ハッシュタグでグループ分け
4. ツイートの多い順にソート
5. ランクを設定

---

### 1.日本語のツイートを抽出 - .small[使うもの]

* `RDD#filter()` を使う
* `filter()` には 「`String` を引数にとって `Boolean` を返す関数」を渡す
  * .accent[true] になる要素だけの RDD が生成される

.footnote[[Scala API - RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)]

* `def containsJapaneseChar(s: String): Boolean`<br> を用意しておきました

▶ `tweetsRDD` に `filter()` を適用してみる

---

### 1.日本語のツイートを抽出 - .small[実装]

~~~scala
  // tweets.txt の中にある全ツイートの RDD
  val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // 1.日本語のツイートを抽出
* val japaneseTweetsRDD: RDD[String] =
*   tweetsRDD.filter(containsJapaneseChar)

  // Ranking に変換する RDD
  val rankingsRDD: RDD[Ranking] =
    tweetsRDD map { tweet: String =>
      // String を Ranking ケースクラスに変換
      Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
    }
~~~
* `tweetsRDD.filter()` に `containsJapaneseChar`<br>を渡す

---

.todo[
* 実装後、`./activator backend/print` で確認
]

### 1.日本語のツイートを抽出 - .small[実装]

~~~scala
  // tweets.txt の中にある全ツイートの RDD
  val tweetsRDD: RDD[String] = sc.textFile(tweetsFilePath)

  // 1.日本語のツイートを抽出
  val japaneseTweetsRDD: RDD[String] =
    tweetsRDD.filter(containsJapaneseChar)

  // Ranking に変換する RDD
  val rankingsRDD: RDD[Ranking] =
*   japaneseTweetsRDD map { tweet: String =>
      // String を Ranking ケースクラスに変換
      Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
    }
~~~
* `Ranking` へは `japaneseTweetsRDD` から変換するように書き換えておく

---

### 2.ハッシュタグを抽出 - .small[設計]

~~~scala
Ranking("#hashTag", rank = ???, Array(tweet), sampleCount = ???)
~~~

.accent[ハッシュタグ]とそのハッシュタグを含んでいる .accent[ツイートの配列]のペアを作らないといけない

???

Ranking: フロントエンドに結果として返すオブジェクト

---

### 2.ハッシュタグを抽出 - .small[設計]

.zero-margin[
~~~scala
// japaneseTweetsRDD: RDD[String]
"ツイートA #hashTag1"
"ツイートB #hashTag1 #hashTag2"
~~~
.indent[▼ .small[ハッシュタグとツイートのペアを作成]]
~~~scala
("#hashTag1", "ツイートA #hashTag1")
("#hashTag1", "ツイートB #hashTag1 #hashTag2")
("#hashTag2", "ツイートB #hashTag1 #hashTag2")
~~~
.indent[▼ .small[同じハッシュタグでグループ分け]]
~~~scala
("#hashTag1", ["ツイートA #hashTag1", "ツイートB #hashTag1 #hashTag2"])
("#hashTag2", ["ツイートB #hashTag1 #hashTag2"])
~~~
.indent[▼ .small[`Ranking`に変換]]
~~~scala
Ranking("#hashTag1", rank = ???, ["ツイートA #hashTag1", "ツイートB #hashTag1 #hashTag2"], sampleCount = ???)
Ranking("#hashTag2", rank = ???, ["ツイートB #hashTag1 #hashTag2"], sampleCount = ???)
~~~
]

---

### 2.ハッシュタグを抽出 - .small[使うもの]
.zero-margin[
~~~scala
// japaneseTweetsRDD: RDD[String]
"ツイートA #hashTag1"
"ツイートB #hashTag1 #hashTag2"
~~~
.indent[▼ .small[ハッシュタグとツイートのペアを作成]]
~~~scala
("#hashTag1", "ツイートA #hashTag1")
("#hashTag1", "ツイートB #hashTag1 #hashTag2")
("#hashTag2", "ツイートB #hashTag1 #hashTag2")
~~~
]
--

* `RDD#flatMap()`, `RDD#map()` を使う
    * for式でシンプルに記述できる
* `def pickHashTags(s: String): Set[String]`<br>を用意しておきました

.footnote[[Scala API - RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)]

---

.todo[
* 実装後、`./activator backend/print` で確認
]

### 2.ハッシュタグを抽出 - .small[実装]

~~~scala
// 2.ハッシュタグを抽出 (flatMap, map を使った場合)
val hashTagTweetPairRDD: RDD[(String, String)] =
  japaneseTweetsRDD flatMap { tweet =>
    pickHashTags(tweet) map { hashTag =>
      (hashTag, tweet)
    }
  }
// 2.ハッシュタグを抽出 (for式を使った場合)
val hashTagTweetPairRDD: RDD[(String, String)] =
  for {
    tweet   <- japaneseTweetsRDD
    hashTag <- pickHashTags(tweet)
  } yield (hashTag, tweet)
~~~
~~~scala
// Ranking に変換する RDD
val rankingsRDD: RDD[Ranking] =
* hashTagTweetPairRDD map { case (hashTag, tweet) =>
*   Ranking(hashTag, rank = 1, Array(tweet), sampleCount = 1)
  }
~~~

---

### 3.ハッシュタグでグループ分け - .small[使うもの]

.zero-margin[
~~~scala
("#hashTag1", "ツイートA #hashTag1")
("#hashTag1", "ツイートB #hashTag1 #hashTag2")
("#hashTag2", "ツイートB #hashTag1 #hashTag2")
~~~
.indent[▼ .small[同じハッシュタグでグループ分け]]
~~~scala
("#hashTag1", ["ツイートA #hashTag1", "ツイートB #hashTag1 #hashTag2"])
("#hashTag2", ["ツイートB #hashTag1 #hashTag2"])
~~~
]
* `RDD#groupByKey()`
    * タプルの1番目をキーとしてグループ分けされる

.footnote[[Scala API - RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)]

---

.todo[
* 実装後、`./activator backend/print` で確認
]

### 3.ハッシュタグでグループ分け - .small[実装]

~~~scala
// 3.ハッシュタグでグループ分け
val hashTagGroupsRDD: RDD[(String, Iterable[String])] =
  hashTagTweetPairRDD.groupByKey()
~~~
~~~scala
// Ranking に変換する RDD
val rankingsRDD: RDD[Ranking] =
* hashTagGroupsRDD map { case (hashTag, tweets) =>
*   Ranking(hashTag, rank = 1, tweets.toArray, sampleCount = tweets.size)
  }
~~~
* `sampleCount`には`tweets.size`を設定しておく

---

.todo[
* 実装後、`./activator backend/print` で確認
]

### 4.ツイートの多い順にソート

* `RDD#sortBy()` を使う
* `tweets.size` の降順にしておく

~~~scala
// 4.ツイートの多い順にソート
val sortedHashTagGroupsRDD: RDD[(String, Iterable[String])] =
  hashTagGroupsRDD.sortBy({ case (_, tweets) =>
    tweets.size
  }, ascending = false)
~~~
~~~scala
// Ranking に変換する RDD
val rankingsRDD: RDD[Ranking] =
* sortedHashTagGroupsRDD map { case (hashTag, tweets) =>
    Ranking(hashTag, rank = 1, tweets.toArray, sampleCount = tweets.size)
  }
~~~

.footnote[[Scala API - RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)]

---

.todo[
* 実装後、`./activator backend/print` で確認
]

### 5.ランクを設定

* `RDD#zipWithIndex()`
* index は 0 始まりなので + 1 しておく

~~~scala
// 5.ランクを設定
val rankedHashTagGroupsRDD: RDD[((String, Iterable[String]), Long)] =
  sortedHashTagGroupsRDD.zipWithIndex()
~~~
~~~scala
// Ranking に変換する RDD
val rankingsRDD: RDD[Ranking] =
* rankedHashTagGroupsRDD map { case ((hashTag, tweets), index) =>
*   Ranking(hashTag, rank = index + 1, tweets.toArray, sampleCount = tweets.size)
}
~~~

.footnote[[Scala API - RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)]

???

終了: 【0:23:00】

---

## 解答

実装が間に合わなかった場合は下記コマンドで 解答をチェックアウトしてください

~~~bash
# spark-hands-on-development ディレクトリの直下で実行してください
git add modules/backend/src/main/scala/com/example/tagrank/backend/spark/SparkLogic.scala
git commit -m "「Sparkでプログラミング ①」の途中まで実装"
git checkout step2
~~~
.small[※ 途中の作業は `step1` ブランチに保存されます]

---
class: middle, center

# Sparkでプログラミング ②
## ストリームデータを処理する

???

#【0:25:00】

---

## 現在の実装

~~~scala
/**
 * ② Spark Streams を使ってリアルタイムにツイートを解析
 */
def analyzeRankingWithStream(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  // Twitter の DStream
  val twitterStream: ReceiverInputDStream[Status] =
    TwitterUtils.createStream(ssc, None)

  // ツイート の DStream に変換
  val tweetStream: DStream[String] =
    twitterStream map { status =>
      status.getText
    }

  // ストリームの塊を処理する
  tweetStream.foreachRDD { rdd: RDD[String] =>
  /* …省略… */
}
~~~

???

`analyzeRankingWithStream`: `analyzeRanking`の下

--

.float-bottom-60.glass-deep[
* DStream の API を通してストリームを操作する
* `foreachRDD`でストリームから`RDD`を取り出せる
]

---

## DStream ? - Discretized Stream

* 離散化ストリーム
* 連続するRDDの集合
    * RDDはバッチインターバルごとの入力データを持つ
    * バッチインターバルは`StreamingContext`に設定
* RDDと似たAPIを持つ

.footnote[[Apache Spark - Discretized Streams (DStreams)](http://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams)([Scala API](https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/streaming/dstream/DStream.html))]

???

* バッチインターバル: 500ミリ秒

---

## 現在の実装

~~~scala
/**
 * ② Spark Streams を使ってリアルタイムにツイートを解析
 */
def analyzeRankingWithStream(sc: SparkContext, ssc: StreamingContext, receiver: ActorSelection): Unit = {

  /* …省略… */

  // ストリームの塊を処理する
  tweetStream.foreachRDD { rdd: RDD[String] =>
      // Ranking に変換する RDD
      val rankingsRDD = rdd map { tweet: String =>
          // String を Ranking ケースクラスに変換
          Ranking("#hashTag", rank = 1, Array(tweet), sampleCount = 1)
      }
      // collect() を呼び出すことによって実際の RDD の処理が始まる
      val rankings = rankingsRDD.collect()

      receiver ! rankings
    }

  // start() を呼び出すことによって上記で定義した Stream の処理が始まる
  ssc.start()
}

~~~

--

.float-bottom-50.glass-deep[
* `foreachRDD`で取り出したRDDには バッチインターバルのデータが含まれている
* バッチインターバル以上の時間幅の集計も可能
]

---

## バッチインターバル以上の集計

.zero-margin[
~~~scala
// 3.ハッシュタグでグループ分け
val hashTagGroupsStream: DStream[(String, Iterable[String])] =
  hashTagTweetPairStream.groupByKey()
~~~
* ⇒ バッチインターバル(500ミリ秒) ごとのツイートが集計される
* 過去何分かの集計を表示したい場合は `DStream#groupByKeyAndWindow`を使って ウィンドウ集計する。↓
~~~scala
// 3.ハッシュタグでグループ分け
val hashTagGroupsStream: DStream[(String, Iterable[String])] =
  hashTagTweetPairStream
      .groupByKeyAndWindow(windowDuration = Minutes(1), slideDuration = Seconds(1))
~~~
]

???

静的なファイルの解析のときを思い出して、集計処理は groupByKey 使いましたよね？

---

## ウィンドウ集計

.size-90[![](img/streaming-window.svg)]

???

* 時間に任意の幅を持たせて集計できるようになる
* 例えば「過去1分間のハッシュタグのランキングを1秒ごとに求めたい」
    * ⇒ window幅 に Minutes(1), slide幅 に Seconds(1) を指定する
    * window幅とslide幅はバッチインターバルの倍数である必要がある
* ブルーの箱の幅がバッチインターバル
* 薄いオレンジの破線の枠が最初の集計グループ
* 濃いオレンジの枠が次の集計グループ


---

## 実装してみよう

1. `DStream#filter`で日本語のツイートを抽出
2. `for` もしくは `DStream#map`と`DStream#flatMap` でハッシュタグ・ツイートのペアを作成
3. `DStream#groupByKeyAndWindow`で 過去1分間の集計を1秒ごとに行う.small[(以降の処理は`DStream#foreachRDD`の中に実装)]
4. `RDD#sortBy()`でツイートの多い順にソート
5. `RDD#zipWithIndex()`でランクを設定

???

終了: 0:42:00

---

.todo[
* 実装後、`./activator backend/print` で確認
]

## 実装してみよう

`analyzeLogic`を書き換えて確認してください

.zero-margin[
~~~scala
val analyzeLogic: analyzeLogicType = analyzeRanking
~~~
.center[▼]
~~~scala
val analyzeLogic: analyzeLogicType = analyzeRankingWithStream
~~~
]

???

* `analyzeLogic`何行目?
    * 19行目

---


## 解答

実装が間に合わなかった場合は下記コマンドで 解答をチェックアウトしてください

~~~bash
# spark-hands-on-development ディレクトリの直下で実行してください
git add modules/backend/src/main/scala/com/example/tagrank/backend/spark/SparkLogic.scala
git commit -m "「Sparkでプログラミング ②」の途中まで実装"
git checkout step3
~~~
.small[※ 途中の作業は `step2` ブランチに保存されます]

---
class: middle, center

# Sparkのクラスタを作る

???

#【0:45:00】

---

## Sparkのクラスタアーキテクチャ

.left-column[
.size-100[![](img/cluster-overview.png)]

.footnote[http://spark.apache.org/docs/latest/cluster-overview.html]
]
.right-column.small[
* Driver Program
    * アプリケーションのメインプロセス。SparkContextを起動する。
* SparkContext
    * クラスタへアクセスするための入り口となるオブジェクト。これからRDDを作る。
* Cluster Manager
    * クラスタのリソースを管理する。
* Worker Node
    * アプリケーションコードが実行されるノード。
* Executor
    * ワーカーノード上で起動するプロセス。Taskを実行したり中間データを保持する。
* Task
    * Executor上で実行される処理。
]

???

http://spark.apache.org/docs/latest/spark-standalone.html#launching-spark-applications

1. Driver Program から filter, map などのタスクが Worker Node の Executor に送られます
1. Worker上の Executor プロセスで Task が実行されて
1. 結果が Spark Context に返されます
1. Cluster Manager が処理のジョブスケジューリングを行います

* Driver は ワーカーノードのどれかで起動する

---

## 手順

1. バックエンドの JAR(Java Archive) を作成
1. Sparkのクラスタを起動
    * Master
    * Worker
1. JAR を Spark のクラスタへ submit

---

.todo[
* ビルド後、JARファイルを確認
]

## バックエンドのJARを作成

```bash
cd spark-hands-on-development
# JAR を作成
./activator backend/assembly

# 下記のログが出力されるとビルド完了
[info] Packaging /Users/kazuki/workspace/spark-hands-on-development/modules/backend/target/scala-2.10/twitter-hashtag-ranking-backend-assembly-1.0.0.jar ...
[info] Done packaging.
[success] Total time: 54 s, completed 2015/11/14 14:29:37

# modules/backend/target/scala-2.10/ に JAR が作成できていることを確認
```
.small[▶ twitter-hashtag-ranking-backend-assembly-1.0.0.jar]

---

.todo[
* Masterを起動
]

## クラスタの起動 - Master

```bash
spark-class org.apache.spark.deploy.master.Master --ip 127.0.0.1

# 下記のログが出力されると起動完了
*15/10/10 00:00:00 INFO Master: Starting Spark master at spark://127.0.0.1:7077
15/10/10 00:00:00 INFO Master: Running Spark version 1.5.2
15/10/10 00:00:00 INFO Utils: Successfully started service 'MasterUI' on port 8080.
15/10/10 00:00:00 INFO MasterWebUI: Started MasterWebUI at http://192.168.179.3:8080
15/10/10 00:00:00 INFO Utils: Successfully started service on port 6066.
15/10/10 00:00:00 INFO StandaloneRestServer: Started REST server for submitting applications on port 6066
15/10/10 00:00:00 INFO Master: I have been elected leader! New state: ALIVE
```
* 起動時のログに Master のアドレスが出力される

???

Masterを複数立ち上げる場合は Zookeeper を使う
https://spark.apache.org/docs/latest/spark-standalone.html#standby-masters-with-zookeeper

---

.todo[
* Workerを起動
]

## クラスタの起動 - Workers

```bash
spark-class org.apache.spark.deploy.worker.Worker spark://127.0.0.1:7077

# ↓のログが出力されると起動完了
15/10/10 00:00:00 INFO Worker: Successfully registered with master spark://127.0.0.1:7077
```

* 引数にMasterのアドレスを指定する
* Workerはいくつでも立ち上げることができる

---

.todo[
* JARをsubmit
]

## クラスタへの submit

```bash
# JAR があるディレクトリに移動
cd spark-hands-on-development/modules/backend/target/scala-2.10/
# Sparkのクラスタへ submit
spark-submit --deploy-mode cluster --master spark://127.0.0.1:7077 twitter-hashtag-ranking-backend-assembly-1.0.0.jar

# *Master* のコンソールに下記のようなログが出力されると完了
15/10/10 00:00:00 INFO Master: Registering app twitter-hashtag-ranking
15/10/10 00:00:00 INFO Master: Registered app twitter-hashtag-ranking with ID app-2015101000000-0000
15/10/10 00:00:00 INFO Master: Launching executor app-2015101000000-0000/0 on worker worker-2015101000000-192.168.179.3-55486
```

---

.todo[
* リアルタイムにランキングが更新されていることを確認
]

## 画面を確認

フロントエンドの起動.small[(既に起動している場合は不要)]
~~~bash
cd spark-hands-on-development
./activator run

# ↓のログが出力されると起動完了
--- (Running the application, auto-reloading is enabled) ---

[info] p.c.s.NettyServer - Listening for HTTP on /0:0:0:0:0:0:0:0:9000

(Server started, use Ctrl+D to stop and go back to the console...)
~~~

▶ブラウザで http://localhost:9000/ を開く

---

## クラスタマネージャーの種類

* [Spark Standalone](http://spark.apache.org/docs/latest/spark-standalone.html)
    * Sparkのパッケージに含まれるクラスタマネージャ。 簡単にクラスタを構築できる。
* [Apache Mesos](http://spark.apache.org/docs/latest/running-on-mesos.html)
    * 汎用クラスタマネージャ。
* [Hadoop YARN](http://spark.apache.org/docs/latest/running-on-yarn.html)
    * Hadoopのリソースマネージャ。

.footnote[[Cluster Manager Types](http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types)]
---

## まとめ

* 分散処理が簡単にプログラミングできる
* ライブラリを組み合わせることで実現できることの 幅が広がる
* プログラムを書き換えることなくスケールアウト できる

---
class: middle, center
# 参考情報

???

#【0:55:00】

* 日本語の書籍紹介

---

## 書籍: 初めてのSpark

.center[.size-30[![](img/learning-spark.jpg)]]
.footnote[https://www.oreilly.co.jp/books/9784873117348/]

???

---

## 書籍: Apache Spark入門

.center.size-30[![](img/spark-japanese-book.jpg)]
.footnote[https://www.shoeisha.co.jp/book/detail/9784798142661]

---

## TIS㈱ リアクティブ・システム<br>コンサルティングサービス

* Typesafe のコンサルティングパートナー
* Reactive Platform .small[(Play / Akka / Slick / Scala)]
  * 技術検証
  * 設計レビュー
  * コードレビュー
  * システム構築

.footnote[
.small[※ Spark は近い将来ラインナップします！！]

https://www.tis.jp/service_solution/goreactive/
]

---
class: middle, center

# Thank you!

---

## お願い

.center[
### アンケートにご協力ください
.qr-code.size-20[http://goo.gl/forms/bPIkjJqhfi]<br>
[回答する](http://goo.gl/forms/bPIkjJqhfi)
]

</textarea>

<script>
  var slideshow = remark.create({
          highlightStyle: 'monokai',
          highlightLanguage: 'remark',
          highlightLines: true
        }) ;

  $(function() {
      // 全てのリンクが別のタブで開くようにする
      $('a').attr({target: "_blank"}); //_

      // QR-Code support
      $(".qr-code").each(function() {
        var $this = $(this);
        var text = $this.text();
        $this.text("");
        $this.qrcode(text);
      });

      // var remote = remark.remote(slideshow, "ws://localhost:9000/");
      var remote = remark.remote(slideshow, "wss://stark-dusk-4378.herokuapp.com/");
      console.log(remote);
      remote.ondisconnect = function() {
        alert("通信が切れました。同期を有効にするにはリロードする必要があります。");
      };

      if (window.location.search && window.location.search == "?control=true") {
        // Controllers
        $(".subscribe-control").hide();
        var passcode = window.prompt("passcode?", "");
        if (passcode === null) {
          return;
        }
        var controlling = remote.control(passcode);

        $(".todo").each(function(todo_index) {
          var $this = $(this);
          var category = $this.attr("class").replace(/\s/g, "_");
          $this.find("li").each(function(li_index) {
            var $li = $(this);
            var id = category + "_" + todo_index + "_" + li_index;
            var label = $li.text();
            $li.text("");
            $li.attr("id", id);
            var $count = $("<span class='count'>0</span>");
            var $population = $("<span class='population'>0</span>");
            var $start = $("<input type='button' value='?' />")
            $li.append($start);
            $li.append($count);
            $li.append("<span>/</span>");
            $li.append($population);
            $li.append("<span>: " + label + "</span>");
            $start.on("click", function() {
              controlling.survey(id);
            });
          });
        });

        controlling.onstartsurvey = function(category) {
          var $result = $(".todo #" + category);
          $result.find(".count").text(0);
          $result.find(".population").text(0);
        };

        controlling.onsurveyresult = function(category, answer) {
          var $result = $(".todo #" + category);
          var $count = $result.find(".count");
          var $population = $result.find(".population");

          if (answer == "true") {
            $count.text(parseInt($count.text()) + 1);
          }
          $population.text(parseInt($population.text()) + 1);
        };

      } else {
        // Follower
        var following = remote.follow();
        var $enableSync = $("#enable-sync");
        following.followWhen = function() {
          return $enableSync.prop("checked");
        };
        $enableSync.click(function() {
          if ($enableSync.prop("checked")) {
            following.sync();
          }
        });
        following.ondefiance = function() {
          $enableSync.prop("checked", false);
        };

        following.questionnaireAnswer = function(category) {
          return $("input#" + category).prop("checked").toString();
        };

        $(".todo").each(function(todo_index) {
          var $this = $(this);
          var category = $this.attr("class").replace(/\s/g, "_");
          $this.find("li").each(function(li_index) {
            var $li = $(this);
            var id = category + "_" + todo_index + "_" + li_index;
            var $input = $("<input type='checkbox' id='" + id + "' />");
            var label = $li.text();
            $li.text("");
            $li.append($input);
            $li.append("<label for=" + id + ">" + label + "</label>");
          });
        });
      }
  });

</script>
</body>
</html>
